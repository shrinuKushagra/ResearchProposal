\documentclass[11pt]{article}
\usepackage{geometry}
\usepackage{color}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{enumitem}
\newcommand{\mc}{\mathcal}

\title{\textbf{Research Proposal}}
\author{Shrinu Kushagra\\
\textit{PhD Candidate, University of Waterloo}}
\date{}

\begin{document}
\maketitle

\section{Introduction}
Very recently, Uber started its self-driving car pickup services in Pittsburg. The guardian reports that by $2020$ most of our cars will be driven by computers and humans will have a permanent backseat. Our phones are now becoming smarter. Apple's siri, for example, can converse like a human-being. Amazon's echo can answers questions, read audio-books, report traffic, provide sports scores among various other things. It has been reported that by 2025, professional jobs like accountants, lawyers, doctors could be done by or be heavily-assisted by machines. Some small startups have already started building programs that could automate all your accounting needs. These are just some of the many ways in which `Artificial Intelligence' is transforming our lives.\\

Computers `learn' in two ways. The first class of programs (or algorithms) are called \textit{supervised} methods where the algorithm learns to identify patterns (like sound or text of the alphabet 'A') with the help of a human being.  The second class of methods are \textit{unsupervised} where the algorithms learns to detect and distinguish patterns by themselves. Lot of the recent success in AI like autonomous cars etc. can be attributed to advances in unsupervised learning algorithms. \\

However, research in unsupervised learning has still not been able to explain the success of some of these algorithms. Under what situations should one algorithm be preferred over the other? What conditions are needed for a certain algorithm succeed? Even more importantly, under what conditions would the given algorithm fail? Current research has little to no answer to these questions. These questions become very critical, like in the domain of self-driving cars, where human lives are at stake. In April 2016, EU even approved a law giving its citizens a \textit{right to explanation} of algorithmic decisions. My research aims to answer the above questions and fill this knowledge gap. I am applying for this scholarship  to support my research on \textbf{Foundations of clustering}, a popular class of unsupervised learning algorithms. 

\section{Research Specifics}
\subsection{Background}
Clustering refers to a popular class of unsupervised learning algorithms. Let us assume that we are given a list of objects. The list of objects could be images, astronomical data (measurements from planets, stars etc.), DNA sequences, users of an online service, outputs from an industrial process or something else. Given such a list, the goal of clustering is to group `similar' objects together and to separate dissimilar objects.

\subsection{Challenges}
\label{subsection:challenges}
The task of clustering is often \textit{under-specified}. It is challenging due to the following reasons.
\begin{itemize}
\item \textit{Multiple solutions} - Consider the problem of clustering users from a medical database. The output of the clustering algorithm can be used to identify patients who have similar symptoms, say for diagnostic purposes or to identify patients who have similar treatment complexity/costs, say for insurance purposes. Diseases with completely different symptoms could have similar costs and diseases with similar symptoms could have very different costs due to gender, race, genetic factors.  Hence, the same data should be clustered in different ways depending upon the application. 
\item \textit{Computationally expensive} - Even in cases when a single solution exists, finding that solution is not easy. Consider the problem of clustering a list of size, say one million. Let's say we want to find three groups in this list. Even in such a simple case, the number of possibilities to consider is $\sim 10^{18}$. A naive clustering algorithm would take atleast thousand years to complete even on the fastest of computers. Mathematicians have defined a notion called \textit{NP-Hard} to characterize such difficult problems. Indeed, clustering has been proven to be NP-Hard under many natural computational models.
\item \textit{Metric} - Another challenge in clustering is defining the notion of \textit{similarity}. Given any pair of objects, how do we measure how similar or dissimilar one object is to another. Without such a measure, there is no hope to cluster the entire list. In a lot of applications, standard mathematical notions of similarity like euclidean-distance, manhattan-distance etc. suffice. However, sometimes custom measures of similarity have to be designed/supplied by the user. In this proposal, we will assume that a suitable metric has been provided by the user and focus on the first two challenges. 
\end{itemize} 

In Section \ref{section:literaturesurvey}, I will discuss the methods that have been used in tackling the above challenges and point out some of the missing pieces. In Section \ref{section:methodology}, I will discuss the progress I have made in tackling these challenges. In Section \ref{section:timeline}, I will present a more detailed discussion on some of the open questions that I plan tackle in the immediate future.

\section{Literature Survey}
\label{section:literaturesurvey}
The goal of clustering algorithm is to partition a list of objects (data) into groups of similar objects. More formally, a clustering algorithm does the following.
\begin{align*}
&\text{Input: A list of objects }\mc X\text{ and the number of groups (or clusters) to output }k\\
&\text{Output: A partition of }\mc X\text{ into clusters }X_1, \ldots, X_k.
\end{align*}
As discussed in the previous section, it is computationally expensive to find the best clustering. Researchers have tried to get around this problem in the following way. A cost is associated with every possible grouping (or clustering) of the list $\mc X$. The goal is now to find the grouping of minimum cost. This grouping is sometimes referred to as the \textit{optimal clustering}. Two popular methods which use this technique are $k$-means {\color{red} [???]} and the $k$-median {\color{red} [???]} clustering algorithm. $k$-means is by far the most popular clustering algorithm. However, these techniques still do not solve the challenges listed in Section \ref{subsection:challenges}. 
\begin{itemize}
\item Researchers have shown that finding the optimal solution to both the $k$-means and the $k$-median cost is NP-Hard {\color{red} [???]}.  Informally, this means that there exist lists such that finding the optimal clustering of these lists is \textbf{computationally expensive}.  No algorithm is known which can find the clustering in a reasonable amount of time. 
\item Researchers have also shown that the $k$-means and $k$-median can sometimes output counter-intuitive groupings {\color{red} [???]}. These algorithms can not capture \textbf{multiple solutions}.
\end{itemize}
Despite these challenges, the $k$-means and $k$-median algorithms have been employed successfully in practice. Some applications include cancerous data detection, search engines etc. {\color{red} [???]}. Researchers have examined this gap between theory and practice. One hypothesis that has recently gained traction is that ``Clustering is difficult only when it doesn't matter" or the CDNM thesis {\color{red} [???]}. This means that the data on which clustering algorithms are computationally expensive do not occur in practice. Researchers have characterized real-world data sets by defining mathematical notions of \textit{clusterable} data. As a result, provably efficient clustering algorithms can be found for these so called {\em nice} data.  

\subsection{Clusterability notions}
In the past few years, there has been a line of work on defining notions of clusterability. The goal of all these methods has been to show that clustering is computationally efficient if the input $\mc X$ enjoys some nice structure. In {\color{red} [???]\cite{bilu2012stable}}, a clustering instance is considered to be \emph{stable} if the optimal clustering to a given cost function does not change under small multiplicative perturbations of distances between the objects. {\color{red} [???]\cite{ackerman2009clusterability}} considered additive perturbations instead of multiplicative.  Using these assumptions, these researchers designed efficient algorithms that finds a clustering with near-optimal cost. 

In terms of clusterability conditions, the most relevant previous papers are those addressing clustering when the input satisfies {\em $\alpha$-center proximity}. Informally, this condition says that the different clusters in the data are well-separated and the parameter $\alpha$ controls the degree of separation. The larger the $\alpha$, the more separated the clusters are. {\color{red} [???]\cite{awasthi2012center}} provide an efficient algorithm when the optimal solution satisfies the $(\alpha > 3)$-center proximity. {\color{red} [???]\cite{balcan2012clustering}} improve this result to $(\alpha = \sqrt{2} + 1 \approx 2.4)$. In {\color{red} [???]\cite{ben2014data}} it was shown that finding the optimal solution for $(\alpha <2)$-center proximal inputs is NP-Hard. To summarize, researchers have shown that when the clusters in the input data are well-separated ($\alpha > 2.4$), it is possible to find the optimal clustering. However, if the separation is reduced beyond a certain point ($\alpha < 2$), the problem again becomes computationally expensive. This line of research is very encouraging. However, some very important questions have still not been answered. 
\begin{itemize}[nolistsep]
\item How natural is the $\alpha$-center proximity condition?
\item Given any dataset, can we verify if the data satisfies this condition or not (for $\alpha > 2.4$)? If it does, then we can apply the known algorithms and efficiently obtain a provably optimal solution. 
\item Are there some natural datasets which satisfy the $\alpha$-center proximity condition? 
\end{itemize}

\vspace{0.1in}\noindent One of the reasons, why no natural datasets satisfying center proximity have been found could be because they impose very strict restrictions on the data. Imagine collecting billions of data-points for your application. Say, we are trying to group millions of patients from a medical dataset based on their symptoms. It is reasonable to expect that such a dataset would have different groups of patient (possibly based on their diseases). However, it is unreasonable to expect each of our million patients to fall exactly in one of the groups. In realistic situations, one would expect that our data has groups which we need to detect. However, some of the points may be such that they do not belong `strongly' to any one group. Such points can be referred to as noise. 

The results we have discussed so far apply only to the noiseless case. I am aware of only two works, {\color{red} [???]\cite{balcan2012clustering}} and {\color{red} [???]\cite{ben2014clustering}} which work in the noise setting. Few methods have been suggested for analyzing clusterability in the presence of noise. However, both of these methods impose very strong restrictions on the noise and clusterability of the data. {\color{red} [???]\cite{balcan2012clustering}} can handle noise only when it is a very small fraction of the whole data (say $4\%$). {\color{red} [???]\cite{ben2014clustering}} requires that the clusters in the data be `hugely' separated. In Section \ref{section:methodology}, I will discuss some of my recent papers which address this issue.

\subsection{Supervised clustering}
As discussed in Section \ref{subsection:challenges}, one of the main challenges in clustering is handling ambiguity. The same data needs to be clustered in different ways depending upon the intended application. One of the open questions in clustering is 
\begin{center}
How do we incorporate domain-knowledge into the clustering task?
\end{center}

The solution is to use some form of human or user supervision. Some of the ways in which researchers have tried to incorporate this information are as follows.
\begin{itemize}[nolistsep]
\item {\em Constraints} -  Let us consider the same application of clustering patients. In this case, a doctor (based on his experience and expertise) would provide us with the following constraints. For example, patients A and B should always be grouped together, patients A and C should never be grouped together etc. Some researchers  ({\color{red} [???]\cite{basu2002semi,basu2004probabilistic, kulis2009semi}}) have adopted this approach.
\item {\em Sample clustering} - {\color{red} [???]\cite{ashtiani2015representation}} first chose a small sample from the original input dataset. The user or expert is then asked to cluster this dataset. Based on the clustering provided by the user, the algorithm then tries to cluster the whole dataset. 
\item {\em Interactive clustering} - Researchers {\color{red} [???]\cite{balcan2008clustering}} considered the following framework. At every step of the algorithm, the expert (like doctor) is provided with the current clustering (grouping), and tells the algorithm to either split a cluster or merge two clusters.
\end{itemize}

\vspace{0.1in}\noindent An important consideration in these works is to minimize the supervision while still incorporating domain-knowledge into the algorithm. For example, a human (doctor) can not be expected to evaluate a clustering at every step and suggest which groups to merge and which to split. Similarly, even clustering thousand objects by an expert could potentially be very expensive. In Section \ref{section:methodology}, I will discuss some of my recent papers which propose a much more realistic method of incorporating user-feedback into the algorithms.


\section{Methodology}
\label{section:methodology}
In the previous section
\subsection{Realistic clusterability notion}
The discussion of finding clustering structure in data sets that also contain subsets that do not conform well to that structure usually falls under the terminology of noise robustness (see e.g., \cite{balcan2012clustering},\cite{ackerman2009clusterability},\cite{dave1993robust}, \cite{cuesta1997trimmed},\cite{garcia2008general}). However, noise robustness, at least in that context, addresses the noisy part of the data as either generated by some specific generative model (like uniform random noise, or Gaussian perturbations) or refers to worst-case adversarially generated noisy data. In this paper we take a different approach. What distinguishes the noise that we consider form the ``clean" part of the input data is that it is \emph{structureless}. The exact meaning of such a notion of structurlessness may vary depending on the type of structure the clustering algorithm is aiming to detect in the data. We focus on defining structurelessness as  not having significantly large dense subsets. we believe that such a notion is well suited to address ``gray background" contrasting with cohesive subsets of the data that are the objects that the clustering aims to detect. 

The distinction between structured and unstructured parts of the data requires, of course, a clear notion of relevant structure. For that, we resort to a relatively large body of recent work proposing notions of clusterable data sets. That work was developed mainly to address the gap between the computational hardness of (the optimization problem of) many common clustering objectives and the apparent feasibility of clustering in practical applications. We refer the reader to \cite{ben2015computational} for a survey of that body of work. Here, we focus on two such notions, one based on the $\alpha$-center-proximity introduced by \cite{awasthi2012center} and the other, $\lambda$-separation, introduced by \cite{ben2014clustering}.

Our approach diverges from previous discussions of clusterable inputs in yet another aspect. Much of the theoretical research of clustering algorithms views clustering as an optimization problem. For some predetermined objective function (or clustering cost), the algorithm's task is to find the data partitioning that minimizes that objective. In particular, this approach is shared by all the works surveyed in \cite{ben2015computational}. We believe that the reality of clustering applications is different. Given a large data set to cluster, there is no way a user may know what is the cost of the optimal clustering of that data, or how close to optimal the algorithm's outcome is. Instead, a user might have a notion of meaningful cluster structure, and will be happy with any outcome that meets such a requirement. Consequently, our algorithms aim to provide meaningful clustering solutions (where ``meaningful" is defined in a way inspired by the above mentioned notions of clusterability) without reference to any particular optimization objective function. Our algorithms efficiently compute a hierarchical clustering tree that captures all such meaningful solutions. One should notice that all of those notions of clusterability (those under which it can be show that an objective-minimizing clustering can be found efficiently) assume that there exists an optimal solution that satisfies the meaningfulness condition (such as being perturbation robust, or having significantly smaller distances of points to their own cluster centers than to other centers). Under those assumptions, an algorithm that outputs a tree capturing all meaningful solutions, allows efficient detection of the cost-optimal clustering (in fact, the algorithms of \cite{balcan2012clustering} also yield such trees, for clean, noiseless inputs). Consequently, under the assumptions of those previous works, our algorithms yield an efficient procedure for finding such an optimal solution.

\subsection{Realistic supervision in clustering}
The two main contributions of this paper are the introduction of the semi-supervised active clustering (SSAC) framework and, the rather unusual 
demonstration that access to simple query answers can turn an otherwise NP hard clustering problem into a feasible one. 

Before we explain those results, let us also mention a notion of clusterability (or `input niceness') that we introduce. We define a novel notion of niceness of data, called $\gamma$-margin property that is related to the previously introduced notion of center proximity (\cite{awasthi2012center}). The larger the value of $\gamma$, the stronger the assumption becomes, which means that clustering becomes easier. With respect to that $\gamma$ parameter, we get a sharp `phase transition' between $k$-means being NP hard and being optimally solvable in polynomial time\footnote{The exact value of such a threshold $\gamma$ depends on some finer details of the clustering task; whether $d$ is required to be Euclidean and whether the cluster centers must be members of $X$.}.

We focus on the effect of using queries on the computational complexity of clustering. We provide a probabilistic polynomial time (BPP) algorithm for clustering with queries, that has guaranteed success under the assumption that the input satisfies the $\gamma$-margin condition for $\gamma > 1$. This algorithm makes $O\big(k^2\log k + k\log n)$ same-cluster queries to the oracle and runs in $O\big(kn\log n)$ time, where $k$ is the number of clusters and $n$ is the size of the instance set.

On the other hand, we show that without access to query answers, $k$-means clustering is NP-hard even when the solution satisfies $\gamma$-margin property for $\gamma=\sqrt{3.4} \approx 1.84$ and $k=\Theta(n^\epsilon)$ (for any $\epsilon\in (0,1)$). We further show that access to $\Omega(\log k + \log n)$ queries is needed to overcome the NP hardness in that case.
These results, put together, show an interesting phenomenon. Assume that the oracle conforms to an optimal solution of $k$-means clustering and that it satisfies the $\gamma$-margin property for some $1<\gamma \leq \sqrt{3.4}$. In this case, our lower bound means that without making queries $k$-means clustering is NP-hard, while the positive result shows that with a reasonable number of queries the problem becomes efficiently solvable.  

%In other words, both the data niceness (margin) condition and the access to query answers are essential to the feasiblity of $k$-means clustering.
This indicates an interesting (and as far as we are aware, novel) trade-off between query complexity and computational complexity in the clustering domain. 


\section{Timeline}
\label{section:timeline}

\section{Conclusion}
\end{document}

%The process is very similar to how a human baby learns (for example writing alphabets). Large amounts of data (say, sounds of different alphabets) is fed to the algorithm along with the `correct' value (text of alphabets). The algorithm then learns to identify and distinguish the different patterns (alphabets).
































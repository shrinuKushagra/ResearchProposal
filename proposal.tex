\documentclass[11pt]{article}
\usepackage{geometry}
\usepackage{color}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{enumitem}
\newcommand{\mc}{\mathcal}

\title{\textbf{Research Proposal}}
\author{Shrinu Kushagra\\
\textit{PhD Candidate, University of Waterloo}}
\date{}

\begin{document}
\maketitle

\section{Introduction}
Very recently, Uber started its self-driving car pickup services in Pittsburg. The guardian reports that by $2020$ most of our cars will be driven by computers and humans will have a permanent backseat. Our phones are now becoming smarter. Apple's siri, for example, can converse like a human-being. Amazon's echo can answers questions, read audio-books, report traffic, provide sports scores among various other things. It has been reported that by 2025, professional jobs like accountants, lawyers, doctors could be done by or be heavily-assisted by machines. Some small startups have already started building programs that could automate all your accounting needs. These are just some of the many ways in which `Artificial Intelligence' is transforming our lives.\\

Computers `learn' in two ways. The first class of programs (or algorithms) are called \textit{supervised} methods where the algorithm learns to identify patterns (like sound or text of the alphabet 'A') with the help of a human being.  The second class of methods are \textit{unsupervised} where the algorithms learns to detect and distinguish patterns by themselves. Lot of the recent success in AI like autonomous cars etc. can be attributed to advances in unsupervised learning algorithms. \\

However, research in unsupervised learning has still not been able to explain the success of some of these algorithms. Under what situations should one algorithm be preferred over the other? What conditions are needed for a certain algorithm succeed? Even more importantly, under what conditions would the given algorithm fail? Current research has little to no answer to these questions. These questions become very critical, like in the domain of self-driving cars, where human lives are at stake. In April 2016, EU even approved a law giving its citizens a \textit{right to explanation} of algorithmic decisions. My research aims to answer the above questions and fill this knowledge gap. I am applying for this scholarship  to support my research on \textbf{Foundations of clustering}, a popular class of unsupervised learning algorithms. 

\section{Research Specifics}
\subsection{Background}
Clustering refers to a popular class of unsupervised learning algorithms. Let us assume that we are given a list of objects. The list of objects could be images, astronomical data (measurements from planets, stars etc.), DNA sequences, users of an online service, outputs from an industrial process or something else. Given such a list, the goal of clustering is to group `similar' objects together and to separate dissimilar objects.

\subsection{Challenges}
\label{subsection:challenges}
The task of clustering is often \textit{under-specified}. It is challenging due to the following reasons.
\begin{itemize}
\item \textit{Multiple solutions} - Consider the problem of clustering users from a medical database. The output of the clustering algorithm can be used to identify patients who have similar symptoms, say for diagnostic purposes or to identify patients who have similar treatment complexity/costs, say for insurance purposes. Diseases with completely different symptoms could have similar costs and diseases with similar symptoms could have very different costs due to gender, race, genetic factors.  Hence, the same data should be clustered in different ways depending upon the application. 
\item \textit{Computationally expensive} - Even in cases when a single solution exists, finding that solution is not easy. Consider the problem of clustering a list of size, say one million. Let's say we want to find three groups in this list. Even in such a simple case, the number of possibilities to consider is $\sim 10^{18}$. A naive clustering algorithm would take atleast thousand years to complete even on the fastest of computers. Mathematicians have defined a notion called \textit{NP-Hard} to characterize such difficult problems. Indeed, clustering has been proven to be NP-Hard under many natural computational models.
\item \textit{Metric} - Another challenge in clustering is defining the notion of \textit{similarity}. Given any pair of objects, how do we measure how similar or dissimilar one object is to another. Without such a measure, there is no hope to cluster the entire list. In a lot of applications, standard mathematical notions of similarity like euclidean-distance, manhattan-distance etc. suffice. However, sometimes custom measures of similarity have to be designed/supplied by the user. In this proposal, we will assume that a suitable metric has been provided by the user and focus on the first two challenges. 
\end{itemize} 

In Section \ref{section:literaturesurvey}, I will discuss the methods that have been used in tackling the above challenges and point out some of the missing pieces. In Section \ref{section:methodology}, I will discuss the progress I have made in tackling these challenges. In Section \ref{section:timeline}, I will present a more detailed discussion on some of the open questions that I plan tackle in the immediate future.

\section{Literature Survey}
\label{section:literaturesurvey}
The goal of clustering algorithm is to partition a list of objects (data) into groups of similar objects. More formally, a clustering algorithm does the following.
\begin{align*}
&\text{Input: A list of objects }\mc X\text{ and the number of groups (or clusters) to output }k\\
&\text{Output: A partition of }\mc X\text{ into clusters }X_1, \ldots, X_k.
\end{align*}
As discussed in the previous section, it is computationally expensive to find the best clustering. Researchers have tried to get around this problem in the following way. A cost is associated with every possible grouping (or clustering) of the list $\mc X$. The goal is now to find the grouping of minimum cost. This grouping is sometimes referred to as the \textit{optimal clustering}. Two popular methods which use this technique are $k$-means {\color{red} [???]} and the $k$-median {\color{red} [???]} clustering algorithm. $k$-means is by far the most popular clustering algorithm. However, these techniques still do not solve the challenges listed in Section \ref{subsection:challenges}. 
\begin{itemize}
\item Researchers have shown that finding the optimal solution to both the $k$-means and the $k$-median cost is NP-Hard {\color{red} [???]}.  Informally, this means that there exist lists such that finding the optimal clustering of these lists is \textbf{computationally expensive}.  No algorithm is known which can find the clustering in a reasonable amount of time. 
\item Researchers have also shown that the $k$-means and $k$-median can sometimes output counter-intuitive groupings {\color{red} [???]}. These algorithms can not capture \textbf{multiple solutions}.
\end{itemize}
Despite these challenges, the $k$-means and $k$-median algorithms have been employed successfully in practice. Some applications include cancerous data detection, search engines etc. {\color{red} [???]}. Researchers have examined this gap between theory and practice. One hypothesis that has recently gained traction is that ``Clustering is difficult only when it doesn't matter" or the CDNM thesis {\color{red} [???]}. This means that the data on which clustering algorithms are computationally expensive do not occur in practice. Researchers have characterized real-world data sets by defining mathematical notions of \textit{clusterable} data. As a result, provably efficient clustering algorithms can be found for these so called {\em nice} data.  

\subsection{Clusterability notions}
In the past few years, there has been a line of work on defining notions of clusterability. The goal of all these methods has been to show that clustering is computationally efficient if the input $\mc X$ enjoys some nice structure. In {\color{red} [???]\cite{bilu2012stable}}, a clustering instance is considered to be \emph{stable} if the optimal clustering to a given cost function does not change under small multiplicative perturbations of distances between the objects. {\color{red} [???]\cite{ackerman2009clusterability}} considered additive perturbations instead of multiplicative.  Using these assumptions, these researchers designed efficient algorithms that finds a clustering with near-optimal cost. 

In terms of clusterability conditions, the most relevant previous papers are those addressing clustering when the input satisfies {\em $\alpha$-center proximity}. Informally, this condition says that the different clusters in the data are well-separated and the parameter $\alpha$ controls the degree of separation. The larger the $\alpha$, the more separated the clusters are. {\color{red} [???]\cite{awasthi2012center}} provide an efficient algorithm when the optimal solution satisfies the $(\alpha > 3)$-center proximity. {\color{red} [???]\cite{balcan2012clustering}} improve this result to $(\alpha = \sqrt{2} + 1 \approx 2.4)$. In {\color{red} [???]\cite{ben2014data}} it was shown that finding the optimal solution for $(\alpha <2)$-center proximal inputs is NP-Hard. To summarize, researchers have shown that when the clusters in the input data are well-separated ($\alpha > 2.4$), it is possible to find the optimal clustering. However, if the separation is reduced beyond a certain point ($\alpha < 2$), the problem again becomes computationally expensive. This line of research is very encouraging. However, some very important questions have still not been answered. 
\begin{itemize}[nolistsep]
\item How natural is the $\alpha$-center proximity condition?
\item Given any dataset, can we verify if the data satisfies this condition or not (for $\alpha > 2.4$)? If it does, then we can apply the known algorithms and efficiently obtain a provably optimal solution. 
\item Are there some natural datasets which satisfy the $\alpha$-center proximity condition? 
\end{itemize}

\vspace{0.1in}\noindent One of the reasons, why no natural datasets satisfying center proximity have been found could be because they impose very strict restrictions on the data. Imagine collecting billions of data-points for your application. Say, we are trying to group millions of patients from a medical dataset based on their symptoms. It is reasonable to expect that such a dataset would have different groups of patient (possibly based on their diseases). However, it is unreasonable to expect each of our million patients to fall exactly in one of the groups. In realistic situations, one would expect that our data has groups which we need to detect. However, some of the points may be such that they do not belong `strongly' to any one group. Such points can be referred to as noise. 

The results we have discussed so far apply only to the noiseless case. I am aware of only two works, {\color{red} [???]\cite{balcan2012clustering}} and {\color{red} [???]\cite{ben2014clustering}} which work in the noise setting. Few methods have been suggested for analyzing clusterability in the presence of noise. However, both of these methods impose very strong restrictions on the noise and clusterability of the data. {\color{red} [???]\cite{balcan2012clustering}} can handle noise only when it is a very small fraction of the whole data (say $4\%$). {\color{red} [???]\cite{ben2014clustering}} requires that the clusters in the data be `hugely' separated. In Section \ref{section:methodology}, I will discuss some of my recent papers which address this issue.

\subsection{Supervised clustering}
As discussed in Section \ref{subsection:challenges}, one of the main challenges in clustering is handling ambiguity. The same data needs to be clustered in different ways depending upon the intended application. One of the open questions in clustering is 
\begin{center}
How do we incorporate domain-knowledge into the clustering task?
\end{center}

The solution is to use some form of human or user supervision. Some of the ways in which researchers have tried to incorporate this information are as follows.
\begin{itemize}[nolistsep]
\item {\em Constraints} -  Let us consider the same application of clustering patients. In this case, a doctor (based on his experience and expertise) would provide us with the following constraints. For example, patients A and B should always be grouped together, patients A and C should never be grouped together etc. Some researchers  ({\color{red} [???]\cite{basu2002semi,basu2004probabilistic, kulis2009semi}}) have adopted this approach.
\item {\em Sample clustering} - {\color{red} [???]\cite{ashtiani2015representation}} first chose a small sample from the original input dataset. The user or expert is then asked to cluster this dataset. Based on the clustering provided by the user, the algorithm then tries to cluster the whole dataset. 
\item {\em Interactive clustering} - Researchers {\color{red} [???]\cite{balcan2008clustering}} considered the following framework. At every step of the algorithm, the expert (like doctor) is provided with the current clustering (grouping), and tells the algorithm to either split a cluster or merge two clusters.
\end{itemize}

\vspace{0.1in}\noindent An important consideration in these works is to minimize the supervision while still incorporating domain-knowledge into the algorithm. For example, a human (doctor) can not be expected to evaluate a clustering at every step and suggest which groups to merge and which to split. Similarly, even clustering thousand objects by an expert could potentially be very expensive. In Section \ref{section:methodology}, I will discuss some of my recent papers which propose a much more realistic method of incorporating user-feedback into the algorithms.

\section{Methodology}
\label{section:methodology}

\section{Timeline}
\label{section:timeline}

\section{Conclusion}
\end{document}

%The process is very similar to how a human baby learns (for example writing alphabets). Large amounts of data (say, sounds of different alphabets) is fed to the algorithm along with the `correct' value (text of alphabets). The algorithm then learns to identify and distinguish the different patterns (alphabets).
































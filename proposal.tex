\documentclass[11pt]{article}
\usepackage{geometry}
\usepackage{color}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{enumitem}
\newcommand{\mc}{\mathcal}

\title{\textbf{Research Proposal}}
\author{Shrinu Kushagra\\
\textit{PhD Candidate, University of Waterloo}}
\date{}

\begin{document}
\maketitle

\section{Introduction}
Very recently, Uber started its self-driving car pickup services in Pittsburg. The guardian reports that by $2020$ most of our cars will be driven by computers and humans will have a permanent backseat. Our phones are now becoming smarter. Apple's siri, for example, can converse like a human-being. Amazon's echo can answers questions, read audio-books, report traffic, provide sports scores among various other things. It has been reported that by 2025, professional jobs like accountants, lawyers, doctors could be done by or be heavily-assisted by machines. Some small startups have already started building programs that could automate all your accounting needs. These are just some of the many ways in which `Artificial Intelligence' is transforming our lives.\\

Computers `learn' in two ways. The first class of programs (or algorithms) are called \textit{supervised} methods where the algorithm learns to identify patterns (like sound or text of the alphabet 'A') with the help of a human being.  The second class of methods are \textit{unsupervised} where the algorithms learns to detect and distinguish patterns by themselves. Lot of the recent success in AI like autonomous cars etc. can be attributed to advances in unsupervised learning algorithms. \\

However, research in unsupervised learning has still not been able to explain the success of some of these algorithms. Under what situations should one algorithm be preferred over the other? What conditions are needed for a certain algorithm succeed? Even more importantly, under what conditions would the given algorithm fail? Current research has little to no answer to these questions. These questions become very critical, like in the domain of self-driving cars, where human lives are at stake. In April 2016, EU even approved a law giving its citizens a \textit{right to explanation} of algorithmic decisions. My research aims to answer the above questions and fill this knowledge gap. I am applying for this scholarship  to support my research on \textbf{Foundations of clustering}, a popular class of unsupervised learning algorithms. 

\section{Research Specifics}
\subsection{Background}
Clustering refers to a popular class of unsupervised learning algorithms. Let us assume that we are given a list of objects. The list of objects could be images, astronomical data (measurements from planets, stars etc.), DNA sequences, users of an online service, outputs from an industrial process or something else. Given such a list, the goal of clustering is to group `similar' objects together and to separate dissimilar objects.

\subsection{Challenges}
\label{subsection:challenges}
The task of clustering is often \textit{under-specified}. It is challenging due to the following reasons.
\begin{itemize}
\item \textit{Multiple solutions} - Consider the problem of clustering users from a medical database. The output of the clustering algorithm can be used to identify patients who have similar symptoms, say for diagnostic purposes or to identify patients who have similar treatment complexity/costs, say for insurance purposes. Diseases with completely different symptoms could have similar costs and diseases with similar symptoms could have very different costs due to gender, race, genetic factors.  Hence, the same data should be clustered in different ways depending upon the application. 
\item \textit{Computationally expensive} - Even in cases when a single solution exists, finding that solution is not easy. Consider the problem of clustering a list of size, say one million. Let's say we want to find three groups in this list. Even in such a simple case, the number of possibilities to consider is $\sim 10^{18}$. A naive clustering algorithm would take atleast thousand years to complete even on the fastest of computers. Mathematicians have defined a notion called \textit{NP-Hard} to characterize such difficult problems. Indeed, clustering has been proven to be NP-Hard under many natural computational models.
\item \textit{Metric} - Another challenge in clustering is defining the notion of \textit{similarity}. Given any pair of objects, how do we measure how similar or dissimilar one object is to another. Without such a measure, there is no hope to cluster the entire list. In a lot of applications, standard mathematical notions of similarity like euclidean-distance, manhattan-distance etc. suffice. However, sometimes custom measures of similarity have to be designed/supplied by the user. In this proposal, we will assume that a suitable metric has been provided by the user and focus on the first two challenges. 
\end{itemize} 

In Section \ref{section:literaturesurvey}, I will discuss the methods that have been used in tackling the above challenges and point out some of the missing pieces. In Section \ref{section:methodology}, I will discuss the progress I have made in tackling these challenges. In Section \ref{section:timeline}, I will present a more detailed discussion on some of the open questions that I plan tackle in the immediate future.

\section{Literature Survey}
\label{section:literaturesurvey}
The goal of clustering algorithm is to partition a list of objects (data) into groups of similar objects. More formally, a clustering algorithm does the following.
\begin{align*}
&\text{Input: A list of objects }\mc X\text{ and the number of groups (or clusters) to output }k\\
&\text{Output: A partition of }\mc X\text{ into clusters }X_1, \ldots, X_k.
\end{align*}
As discussed in the previous section, it is computationally expensive to find the best clustering. Researchers have tried to get around this problem in the following way. A cost is associated with every possible grouping (or clustering) of the list $\mc X$. The goal is now to find the grouping of minimum cost. This grouping is sometimes referred to as the \textit{optimal clustering}. Two popular methods which use this technique are $k$-means \cite{macqueen1967some} and the $k$-median \cite{jain1988algorithms} clustering algorithm. $k$-means is by far the most popular clustering algorithm. However, these techniques still do not solve the challenges listed in Section \ref{subsection:challenges}. 
\begin{itemize}
\item Researchers have shown that finding the optimal solution to both the $k$-means and the $k$-median cost is NP-Hard \cite{dasgupta2008hardness}.  Informally, this means that there exist lists such that finding the optimal clustering of these lists is \textbf{computationally expensive}.  No algorithm is known which can find the clustering in a reasonable amount of time. 
\item Researchers have also shown that the $k$-means and $k$-median can sometimes output counter-intuitive groupings \cite{mirkes2011k}. These algorithms can not capture \textbf{multiple solutions}.
\end{itemize}
Despite these challenges, the $k$-means and $k$-median algorithms have been employed successfully in practice. Some applications include cancerous data detection, search engines etc. \cite{wang2005comparison,liu2007clustering}. Researchers have examined this gap between theory and practice. One hypothesis that has recently gained traction is that ``Clustering is difficult only when it doesn't matter" or the CDNM thesis \cite{daniely2012clustering}. This means that the data on which clustering algorithms are computationally expensive do not occur in practice. Researchers have characterized real-world data sets by defining mathematical notions of \textit{clusterable} data. As a result, provably efficient clustering algorithms can be found for these so called {\em nice} data.  

\subsection{Clusterability notions}
\label{subsection:clusterabilitynotion}
In the past few years, there has been a line of work on defining notions of clusterability. The goal of all these methods has been to show that clustering is computationally efficient if the input $\mc X$ enjoys some nice structure. In \cite{bilu2012stable}, a clustering instance is considered to be \emph{stable} if the optimal clustering to a given cost function does not change under small multiplicative perturbations of distances between the objects. \cite{ackerman2009clusterability} considered additive perturbations instead of multiplicative.  Using these assumptions, these researchers designed efficient algorithms that finds a clustering with near-optimal cost. 

In terms of clusterability conditions, the most relevant previous papers are those addressing clustering when the input satisfies {\em $\alpha$-center proximity}. Informally, this condition says that the different clusters in the data are well-separated and the parameter $\alpha$ controls the degree of separation. The larger the $\alpha$, the more separated the clusters are. \cite{awasthi2012center} provide an efficient algorithm when the optimal solution satisfies the $(\alpha > 3)$-center proximity. \cite{balcan2012clustering} improve this result to $(\alpha = \sqrt{2} + 1 \approx 2.4)$. In \cite{ben2014data} it was shown that finding the optimal solution for $(\alpha <2)$-center proximal inputs is NP-Hard. To summarize, researchers have shown that when the clusters in the input data are well-separated ($\alpha > 2.4$), it is possible to find the optimal clustering. However, if the separation is reduced beyond a certain point ($\alpha < 2$), the problem again becomes computationally expensive. This line of research is very encouraging. However, some very important questions have still not been answered. 
\begin{itemize}[nolistsep]
\item How natural is the $\alpha$-center proximity condition?
\item Given any dataset, can we verify if the data satisfies this condition or not (for $\alpha > 2.4$)? If it does, then we can apply the known algorithms and efficiently obtain a provably optimal solution. 
\item Are there some natural datasets which satisfy the $\alpha$-center proximity condition? 
\end{itemize}

\vspace{0.1in}\noindent One of the reasons, why no natural datasets satisfying center proximity have been found could be because they impose very strict restrictions on the data. Imagine collecting billions of data-points for your application. Say, we are trying to group millions of patients from a medical dataset based on their symptoms. It is reasonable to expect that such a dataset would have different groups of patient (possibly based on their diseases). However, it is unreasonable to expect each of our million patients to fall exactly in one of the groups. In realistic situations, one would expect that our data has groups which we need to detect. However, some of the points may be such that they do not belong `strongly' to any one group. Such points can be referred to as noise. 

The results we have discussed so far apply only to the noiseless case. I am aware of only two works, \cite{balcan2012clustering} and \cite{ben2014clustering} which work in the noise setting. Few methods have been suggested for analyzing clusterability in the presence of noise. However, both of these methods impose very strong restrictions on the noise and clusterability of the data. \cite{balcan2012clustering} can handle noise only when it is a very small fraction of the whole data (say $4\%$). \cite{ben2014clustering} requires that the clusters in the data be `hugely' separated. In Section \ref{section:methodology}, I will discuss some of my recent papers which address this issue.

\subsection{Supervised clustering}
\label{subsection:supervisedclustering}
As discussed in Section \ref{subsection:challenges}, one of the main challenges in clustering is handling ambiguity. The same data needs to be clustered in different ways depending upon the intended application. One of the open questions in clustering is 
\begin{center}
How do we incorporate domain-knowledge into the clustering task?
\end{center}

The solution is to use some form of human or user supervision. Some of the ways in which researchers have tried to incorporate this information are as follows.
\begin{itemize}[nolistsep]
\item {\em Constraints} -  Let us consider the same application of clustering patients. In this case, a doctor (based on his experience and expertise) would provide us with the following constraints. For example, patients A and B should always be grouped together, patients A and C should never be grouped together etc. Some researchers \cite{basu2002semi,basu2004probabilistic} have adopted this approach.
\item {\em Sample clustering} - \cite{ashtiani2015representation} first chose a small sample from the original input dataset. The user or expert is then asked to cluster this dataset. Based on the clustering provided by the user, the algorithm then tries to cluster the whole dataset. 
\item {\em Interactive clustering} - Researchers \cite{balcan2008clustering} considered the following framework. At every step of the algorithm, the expert (like doctor) is provided with the current clustering (grouping), and tells the algorithm to either split a cluster or merge two clusters.
\end{itemize}

\vspace{0.1in}\noindent An important consideration in these works is to minimize the supervision while still incorporating domain-knowledge into the algorithm. For example, a human (doctor) can not be expected to evaluate a clustering at every step and suggest which groups to merge and which to split. Similarly, even clustering thousand objects by an expert could potentially be very expensive. In Section \ref{section:methodology}, I will discuss some of my recent papers which propose a much more realistic method of incorporating user-feedback into the algorithms.


\section{Methodology}
\label{section:methodology}
\subsection{Realistic clusterability notion}
In Section \ref{subsection:challenges}, I discussed that one of the major challenges of clustering was that it is computationally expensive. In \ref{subsection:clusterabilitynotion}, I discussed a recent line of work to address this issue, namely, notions of clusterability. The basic idea is to show that clustering is easy (computationally cheap) if the data has some nice properties. However, lot of these notions are unrelaistic and not likely to occur in real-world situations. The reason for this is that adequate attention has not been paid to clusterability notions which handle noise. Previously, researchers have addressed noise robustness in a very limited context, like uniform random noise or worst-case adversarial noise \cite{balcan2012clustering,ackerman2009clusterability}.

In \cite{kushagra2016finding}, me and my coauthors took a different approach. What distinguishes the noise that we consider form the ``clean" part of the input data is that it is \emph{structureless}. This definition imposes no restriction on the size of the noisy data. A common scenario in real-world applications is that the data has cohesive subsets and a ``gray background''.  This approach diverges from previous discussions of clusterable data in yet another aspect. Much of the theoretical research of clustering algorithms views clustering as an optimization problem. 

In Section \ref{subsection:challenges}, I discussed how this presents the challenge of multiple solutions and uninterpretable solutions. For some predetermined cost function, the algorithm's task is to find the data partitioning (or clustering) that minimizes that cost. The reality of clustering applications is different. Given a large data set to cluster, there is no way a user may know what is the cost of the optimal clustering of that data, or how close to optimal the algorithm's outcome is. Instead, a user might have a notion of meaningful cluster structure, and will be happy with any outcome that meets such a requirement. Consequently, the algorithm proposed in my paper \cite{kushagra2016finding} aims to provide meaningful clustering solutions without reference to any particular optimization objective function. Our algorithms efficiently captures all such meaningful solutions. This work was presented at ALT 2016. 

%One should notice that all of those notions of clusterability (those under which it can be show that an objective-minimizing clustering can be found efficiently) assume that there exists an optimal solution that satisfies the meaningfulness condition (such as being perturbation robust, or having significantly smaller distances of points to their own cluster centers than to other centers). Under those assumptions, an algorithm that outputs a tree capturing all meaningful solutions, allows efficient detection of the cost-optimal clustering (in fact, the algorithms of \cite{balcan2012clustering} also yield such trees, for clean, noiseless inputs). Consequently, under the assumptions of those previous works, our algorithms yield an efficient procedure for finding such an optimal solution.

\subsection{Realistic supervision in clustering}
In Section \ref{subsection:challenges}, I discussed the challenge of multiple solutions. In Section \ref{subsection:supervisedclustering}, I discussed that one of the possible solution was to incorporate domain knowledge into the problem with weak supervision of an expert(human). However, the approaches discussed in Section \ref{subsection:supervisedclustering} were somewhat unrealistic. 

In \cite{ashtiani2016clustering}, me and my co-authors introduced a new framework of human-supervised clustering. An expert is given two points and should answer whether to keep the two points in the same or different cluster. I then show that access to few such answers can turn an otherwise NP hard (computationally expensive) clustering problem into a feasible one. I also show that if the number of queries is less than a certain threshold then the problem still remains computationally infeasible (NP-Hard). 

This work has a lot of practical applications. While a doctor can't be expected to evaluate an entire dataset, it is very reasonable to give him a few pairs of patients and ask if they belong to the same group or not (depending upon our application). This work was accepted at NIPS 2016 and was among the top 46 papers out of 2500 submissions. One of the reviewers had remarked that this work is \textbf{potentially seminal}. 

%Before we explain those results, let us also mention a notion of clusterability (or `input niceness') that we introduce. We define a novel notion of niceness of data, called $\gamma$-margin property that is related to the previously introduced notion of center proximity (\cite{awasthi2012center}). The larger the value of $\gamma$, the stronger the assumption becomes, which means that clustering becomes easier. With respect to that $\gamma$ parameter, we get a sharp `phase transition' between $k$-means being NP hard and being optimally solvable in polynomial time\footnote{The exact value of such a threshold $\gamma$ depends on some finer details of the clustering task; whether $d$ is required to be Euclidean and whether the cluster centers must be members of $X$.}.

%We focus on the effect of using queries on the computational complexity of clustering. We provide a probabilistic polynomial time (BPP) algorithm for clustering with queries, that has guaranteed success under the assumption that the input satisfies the $\gamma$-margin condition for $\gamma > 1$. This algorithm makes $O\big(k^2\log k + k\log n)$ same-cluster queries to the oracle and runs in $O\big(kn\log n)$ time, where $k$ is the number of clusters and $n$ is the size of the instance set.

%On the other hand, we show that without access to query answers, $k$-means clustering is NP-hard even when the solution satisfies $\gamma$-margin property for $\gamma=\sqrt{3.4} \approx 1.84$ and $k=\Theta(n^\epsilon)$ (for any $\epsilon\in (0,1)$). We further show that access to $\Omega(\log k + \log n)$ queries is needed to overcome the NP hardness in that case.
%These results, put together, show an interesting phenomenon. Assume that the oracle conforms to an optimal solution of $k$-means clustering and that it satisfies the $\gamma$-margin property for some $1<\gamma \leq \sqrt{3.4}$. In this case, our lower bound means that without making queries $k$-means clustering is NP-hard, while the positive result shows that with a reasonable number of queries the problem becomes efficiently solvable.  

\section{Timeline}
\label{section:timeline}
So far, I have discussed the challenges in clustering, previous works of various researchers and some of our methods to address those challenges. In this section, I would like to point-out the future problems that I would like to work and the estimated timelines for the same.

\subsection{Clustering realistic data}
Researchers have shown how the task of clustering can become easy if the data has some nice properties. However, such properties are not realistic and a data set with such properties has still not been found. In the previous section, I showed how incorporating `structureless noise' makes our assumptions much more realistic. However, several questions have still not been answered.

\begin{itemize}
\item In \cite{kushagra2016finding},  I showed that if the data has inherent niceness ($\alpha$-center) and the noise is structureless, then it is possible to detect (or recover) the nice structure of the data. If the data has some other form of niceness (weak-deletion stability) or some other form, is it possible to recover the nice structure in that setting?
\item As already mentioned in previous section, clustering is often posed as an optimization problem. The goal is to find a clustering which minimizes some cost function. Can we propose some other cost function which would be more reasonable when working with data which also has some noisy part?
\item Can we find a dataset which has structureless noise and implement our algorithm on a large-scale practical problem?
\end{itemize}
Each of these questions is highly involved and would take about six months of research. My plan is to spend the first year of award, researching and solving the first two questions. I would then like to spend the next year in solving the third question which is much more highly involved.

\subsection{Incorporating domain expertise}
In \cite{ashtiani2016clustering},  me and my co-authors presented a realistic framework of incorporating domain knowledge into a clustering task.  However, several questions have still not been answered.
\begin{itemize}
\item In \cite{ashtiani2016clustering}, me and my-coauthors assumed that the human expert is perfect and does not make mistakes. However, it is possible that the expert (say a doctor) could make few mistakes. It could also be that the expert is unsure and chooses not to answer our query. Can we develop a framework for such a scenario?
\item Can we incorporate domain knowledge if the input has some noisy points?
\item  Can we implement our algorithm on a large-scale practical problem?
\end{itemize}
Addressing the first two questions would again involve atleast six months of effort each. The next year could then be spent on finding the answer to the third question which is more involved. 

\section{Conclusion}
Clustering is a problem which has a wide variety of applications. In the summer 2016, I did an internship with a company called Pinterest in San Francisco. They are building a discovery engine which tries to show related content to its users based on what they have looked at before. For revenue, they show ads to their users. One of the problems they had was to cluster their users so that similar users could be shown similar ads. This would help them make more money.

Common clustering approaches like  k-means or k-median just didnt work for this purpose. Such approaches also lack interpretability. It is very difficult to understand the clustering that these algorithms output. They are computationally very expensive and take a lot of time to learn. Hence, after a few months of effort, this direction was abandoned by the company.

My research aims to fulfil this knowledge gap. It aims to incorporate more user expertise into the algorithm. It aims to make the clustering algorithm more realistic. 

\bibliographystyle{alpha}
\bibliography{proposal} 
\end{document}

%The process is very similar to how a human baby learns (for example writing alphabets). Large amounts of data (say, sounds of different alphabets) is fed to the algorithm along with the `correct' value (text of alphabets). The algorithm then learns to identify and distinguish the different patterns (alphabets).
































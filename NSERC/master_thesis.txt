We live in a world that is data driven. For example, the internet alone creates 2.5 quintillion bytes of data (enough to fill 10 million disks)! Moreover, most of the current data has been generated over the last two years, and it is only going to increase in the future. This poses an interesting question: how do we make sense of all this data, particularly at this scale? For instance, even a 20 x 20 image has 400-pixel intensities and therefore can have potentially 400 dimensions. In some applications, the number of dimensions could be as high as one million.

Not only it is harder to store, but such high dimensional data also renders most well-studied techniques from low dimensions useless. This is often referred to as the "curse of dimensionality" in the machine learning community. However, as one may suspect, not all of these million dimensions offer unique pieces of information. That is, if we are careful enough we may be able to transform the data to fewer dimensions and still preserve relevant information. This is what we call "dimensionality reduction." Various such techniques have been developed and are quite successful in practice, but
there are some interesting unanswered questions. For example, when should one method be preferred over the other? Under what conditions on the data, is a particular method guaranteed to succeed? 

In my master's thesis, we address these questions. We develop a mathematical notion called metric retention and prove that if any dimensionality reduction technique has the metric retention property,
then it is guaranteed to succeed in a broad class of problems. This notion also helped explain the success and prevalence of common techniques such as PCA. It also helped guide the choice of which methods to prefer under which conditions.

Surprisingly, the notion mentioned above has found applications in other areas of machine learning as well. Deep neural networks are perhaps the most wide-used learning algorithm in the last decade.
 They have been used in speech detection, image processing, text classification, etc. and have at times shown better-than-human performance on these tasks. Our notion was also able to explain the success of these algorithms. 

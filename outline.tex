\documentclass[12pt]{article}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{mathptmx}
\usepackage{enumitem}
\usepackage[paperheight=11in,paperwidth=8.5in,margin=0.75in]{geometry}

\usepackage{fancyhdr}
\pagestyle{fancy}
\rhead{Shrinu Kushagra (PIN: 504441)}

\begin{document}
\linespread{0.901}

Computers `learn' in two ways. The first class of programs (or algorithms) are called \textit{supervised} methods where the algorithm learns to identify patterns (like sound or text of the alphabet 'A') with the help of a human being.  The second class of methods are \textit{unsupervised} where the algorithms learns to detect and distinguish patterns by themselves. Lot of the recent success in artificial intelligence like autonomous cars etc. can be attributed to advances in unsupervised learning algorithms. 

Clustering refers to a popular class of unsupervised learning algorithms. Let us assume that we are given a list of objects (dataset). The list of objects could be images (taken by camera attached to an autonomous car), astronomical data (measurements from planets, stars etc.), DNA sequences, users of an online service (like facebook, google etc.), outputs from an industrial process or something else. Given such a list, the goal of clustering is to group `similar' objects together and to separate dissimilar objects. 

The task of clustering is challenging due to the following reasons. The first is the problem of \textit{multiple solutions}. The same dataset needs to be clustered in different ways depending upon the intended application. The second is the \textit{computational cost}. Even in cases when a single solution exists, finding that solution can take a lot of time. Consider the problem of finding three groups in a list of size one million. The number of possibilities to consider is $\sim 10^{18}$. Mathematicians have defined a notion called \textit{NP-Hard} to characterize such difficult problems. Indeed, clustering has been proven to be NP-Hard under many natural computational models.

In my research I aim to address both of these issues. The challenge of multiple solutions can be tackled by incorporating domain knowledge into the clustering task. Some researchers achieved this by asking a human expert to cluster a small subset of the data. Based on this, the algorithm then tries to cluster the whole dataset.  Another framework has also been considered. At every step of the algorithm, the expert (ex. a doctor) is provided with the current clustering (grouping), and tells the algorithm to either split a cluster or merge two clusters. 

An important consideration is to minimize the supervision while still incorporating domain-knowledge into the algorithm. For example, a human expert can not be expected to evaluate a clustering at every step and suggest which groups to merge and which to split. Similarly, even clustering thousand objects by an expert could potentially be very expensive. In a preliminary work, me and my co-authors introduced a new framework of human-supervised clustering. An expert is given two points and should answer whether to keep the two points in the same or different cluster. We then showed that access to few such answers can turn an otherwise NP hard clustering problem into a feasible one. Going forward, I would like to extend this framework to handle situations in which the expert makes a small number of mistakes. Another situation would be when expert itself is confused and chooses not to answer a query. Another major thrust of my research would be to implement our algorithm on a large-scale practical problem. 

While clustering algorithms are NP-Hard, still they have been employed successfully in practice on applications like cancerous data detection, search engines etc. Researchers have examined this gap between theory and practice. One hypothesis that has recently gained traction is that ``Clustering is difficult only when it doesn't matter". This means that the data on which clustering algorithms are computationally expensive do not occur in practice. Researchers have defined mathematical notions of {\em clusterable} and {\em nice} data and provably efficient clustering algorithms have been found for these datasets.

However, lot of these notions (or properties) are unrealistic and impose too many restrictions on the input dataset. Hence, a dataset satisfying these properties is not likely to occur in real-world situations. The reason for this is that adequate attention has not been paid to clusterability notions which handle noise. In a preliminary work, me and my coauthors showed how incorporating `structureless noise' makes our assumptions much more realistic. Going forward, I would like to implement our algorithm on a large-scale practical problem. I would like to extend our framework of structureless noise to handle different notions of clusterability. Clustering is often posed as an optimization problem but doesn't take noise into account. I would like to propose an optimization problem which takes the noise into account too. 

\end{document}
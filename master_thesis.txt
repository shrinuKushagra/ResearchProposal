Every day, the internet creates 2.5 quintillion bytes of data. That is enough to fill ten million blue-ray disks. If these discs were to be stacked on top of one another, then their height would be the same as the height of four Eiffel Towers. Another interesting fact is that 90% of the data was generated in the last two years. With so much data being generated every day and at an ever-increasing rate, storing and analyzing the data is a major challenge. This data, however, is unprocessed. Consider, for example, a simple 20 by 20 image. This image is represented by 400-pixel intensities or 400 dimensions. However, a lot of the dimensions contain redundant information. In some applications, the dimension can be as high as a million.

Not only is it harder to store, but it is also difficult to analyze high dimension data. Owing to these difficulties, the term "curse of dimensionality" is quite popular in machine learning community. Dimensionality reduction, as the name suggests, aims to reduce the dimension of the data. However, the challenge is to accomplish this while preserving as much task-relevant information as possible. For example, if we were to reduce the dimension of a 20 by 20 image from 400 to say 50, it is conceivable that some information might be lost. Various dimensionality reduction techniques are used in practice. When should one method be preferred over the other? Under what conditions on the data, is a particular method guaranteed to succeed? 

In my master's thesis, I addressed these questions. I developed a mathematical notion called metric retention. I proved that if any dimensionality reduction technique had the metric retention property, then it is guaranteed to succeed in a broad class of problems. This notion also helped explain the success and prevalence of common techniques like PCA. It also helped guide the choice of which methods to prefer under which conditions.

Surprisingly, the notion had applications in other areas of machine learning as well. Deep neural networks are perhaps the most wide-used learning algorithm in the last decade. They have been used in speech detection, image processing, text classification, etc. and have at times shown better-than-human performance on these tasks. My notion was also able to explain the success of these algorithms. 

Computers learn in the following manner. In the initial phase, large amounts of data are fed to the algorithm. The algorithm then learns meaningful and useful patterns from the data, very similar to how a human baby learns (say alphabets). The data that is fed to the algorithm has a lot of features. For example, for the task of identifying alphabets, the data being fed are images of the alphabets. Each image is represented by their pixel intensities. So a 20 by 20 image is represented by 400-pixel intensities or 400 features. The number of features in the data is also called the dimension of the data. In many applications of machine learning, the data dimension can be huge, sometimes even in millions. 

High dimension data poses a lot of challenges. The higher the dimension, the slower is the learning algorithm. Not only that, we need more data to learn in high dimension than in a lower dimension. These difficulties are often summarized as the "curse of dimensionality." Hence before actual learning begins, dimensionality reduction techniques are used as a preprocessing step. The goal of these techniques is to reduce the dimension of the data while retaining as much task-relevant information as possible. 

In my master's thesis, we provided some theoretical principles to help explain the success of certain dimensionality reduction techniques as well as to guide the choice of dimensionality reduction tools and parameters. Our analysis is based on formalizing the often implicit assumption that "similar data points are likely to have similar labels." In simpler terms, similar 'looking' alphabets are likely to be the same. Our theoretical analysis is supported by experimental results.

% uWaterloo Thesis Template for LaTeX 
% Last Updated May 24, 2011 by Stephen Carr, IST Client Services
% FOR ASSISTANCE, please send mail to rt-IST-CSmathsci@ist.uwaterloo.ca

% Effective October 2006, the University of Waterloo 
% requires electronic thesis submission. See the uWaterloo thesis regulations at
% http://www.grad.uwaterloo.ca/Thesis_Regs/thesistofc.asp.

% DON'T FORGET TO ADD YOUR OWN NAME AND TITLE in the "hyperref" package
% configuration below. THIS INFORMATION GETS EMBEDDED IN THE PDF FINAL PDF DOCUMENT.
% You can view the information if you view Properties of the PDF document.

% Many faculties/departments also require one or more printed
% copies. This template attempts to satisfy both types of output. 
% It is based on the standard "book" document class which provides all necessary 
% sectioning structures and allows multi-part theses.

% DISCLAIMER
% To the best of our knowledge, this template satisfies the current uWaterloo requirements.
% However, it is your responsibility to assure that you have met all 
% requirements of the University and your particular department.
% Many thanks to the feedback from many graduates that assisted the development of this template.

% -----------------------------------------------------------------------

% By default, output is produced that is geared toward generating a PDF 
% version optimized for viewing on an electronic display, including 
% hyperlinks within the PDF.
 
% E.g. to process a thesis called "mythesis.tex" based on this template, run:

% pdflatex mythesis	-- first pass of the pdflatex processor
% bibtex mythesis	-- generates bibliography from .bib data file(s) 
% pdflatex mythesis	-- fixes cross-references, bibliographic references, etc
% pdflatex mythesis	-- fixes cross-references, bibliographic references, etc

% If you use the recommended LaTeX editor, Texmaker, you would open the mythesis.tex
% file, then click the pdflatex button. Then run BibTeX (under the Tools menu).
% Then click the pdflatex button two more times. If you have an index as well,
% you'll need to run MakeIndex from the Tools menu as well, before running pdflatex
% the last two times.

% N.B. The "pdftex" program allows graphics in the following formats to be
% included with the "\includegraphics" command: PNG, PDF, JPEG, TIFF
% Tip 1: Generate your figures and photos in the size you want them to appear
% in your thesis, rather than scaling them with \includegraphics options.
% Tip 2: Any drawings you do should be in scalable vector graphic formats:
% SVG, PNG, WMF, EPS and then converted to PNG or PDF, so they are scalable in
% the final PDF as well.
% Tip 3: Photographs should be cropped and compressed so as not to be too large.

% To create a PDF output that is optimized for double-sided printing: 
%
% 1) comment-out the \documentclass statement in the preamble below, and
% un-comment the second \documentclass line.
%
% 2) change the value assigned below to the boolean variable
% "PrintVersion" from "false" to "true".

% --------------------- Start of Document Preamble -----------------------

% Specify the document class, default style attributes, and page dimensions
% For hyperlinked PDF, suitable for viewing on a computer, use this:
\documentclass[letterpaper,12pt,titlepage,oneside,final]{book}
 
% For PDF, suitable for double-sided printing, change the PrintVersion variable below
% to "true" and use this \documentclass line instead of the one above:
%\documentclass[letterpaper,12pt,titlepage,openright,twoside,final]{book}

% Some LaTeX commands I define for my own nomenclature.
% If you have to, it's better to change nomenclature once here than in a 
% million places throughout your thesis!
\newcommand{\package}[1]{\textbf{#1}} % package names in bold text
\newcommand{\cmmd}[1]{\textbackslash\texttt{#1}} % command name in tt font 
\newcommand{\href}[1]{#1} % does nothing, but defines the command so the
    % print-optimized version will ignore \href tags (redefined by hyperref pkg).
%\newcommand{\texorpdfstring}[2]{#1} % does nothing, but defines the command
% Anything defined here may be redefined by packages added below...

% This package allows if-then-else control structures.
\usepackage{ifthen}
\newboolean{PrintVersion}
\setboolean{PrintVersion}{false} 
% CHANGE THIS VALUE TO "true" as necessary, to improve printed results for hard copies
% by overriding some options of the hyperref package below.

%\usepackage{nomencl} % For a nomenclature (optional; available from ctan.org)
\usepackage{amsmath,amssymb,amstext} % Lots of math symbols and environments
\usepackage[pdftex]{graphicx} % For including graphics N.B. pdftex graphics driver 

% Hyperlinks make it very easy to navigate an electronic document.
% In addition, this is where you should specify the thesis title
% and author as they appear in the properties of the PDF document.
% Use the "hyperref" package 
% N.B. HYPERREF MUST BE THE LAST PACKAGE LOADED; ADD ADDITIONAL PKGS ABOVE
\usepackage[pdftex,letterpaper=true,pagebackref=false]{hyperref} % with basic options
		% N.B. pagebackref=true provides links back from the References to the body text. This can cause trouble for printing.
\hypersetup{
    plainpages=false,       % needed if Roman numbers in frontpages
    pdfpagelabels=true,     % adds page number as label in Acrobat's page count
    bookmarks=true,         % show bookmarks bar?
    unicode=false,          % non-Latin characters in Acrobat’s bookmarks
    pdftoolbar=true,        % show Acrobat’s toolbar?
    pdfmenubar=true,        % show Acrobat’s menu?
    pdffitwindow=false,     % window fit to page when opened
    pdfstartview={FitH},    % fits the width of the page to the window
    pdftitle={uWaterloo\ LaTeX\ Thesis\ Template},    % title: CHANGE THIS TEXT!
%    pdfauthor={Author},    % author: CHANGE THIS TEXT! and uncomment this line
%    pdfsubject={Subject},  % subject: CHANGE THIS TEXT! and uncomment this line
%    pdfkeywords={keyword1} {key2} {key3}, % list of keywords, and uncomment this line if desired
    pdfnewwindow=true,      % links in new window
    colorlinks=true,        % false: boxed links; true: colored links
    linkcolor=blue,         % color of internal links
    citecolor=green,        % color of links to bibliography
    filecolor=magenta,      % color of file links
    urlcolor=cyan           % color of external links
}
\ifthenelse{\boolean{PrintVersion}}{   % for improved print quality, change some hyperref options
\hypersetup{	% override some previously defined hyperref options
%    colorlinks,%
    citecolor=black,%
    filecolor=black,%
    linkcolor=black,%
    urlcolor=black}
}{} % end of ifthenelse (no else)

% Setting up the page margins...
% uWaterloo thesis requirements specify a minimum of 1 inch (72pt) margin at the
% top, bottom, and outside page edges and a 1.125 in. (81pt) gutter
% margin (on binding side). While this is not an issue for electronic
% viewing, a PDF may be printed, and so we have the same page layout for
% both printed and electronic versions, we leave the gutter margin in.
% Set margins to minimum permitted by uWaterloo thesis regulations:
\setlength{\marginparwidth}{0pt} % width of margin notes
% N.B. If margin notes are used, you must adjust \textwidth, \marginparwidth
% and \marginparsep so that the space left between the margin notes and page
% edge is less than 15 mm (0.6 in.)
\setlength{\marginparsep}{0pt} % width of space between body text and margin notes
\setlength{\evensidemargin}{0.125in} % Adds 1/8 in. to binding side of all 
% even-numbered pages when the "twoside" printing option is selected
\setlength{\oddsidemargin}{0.125in} % Adds 1/8 in. to the left of all pages
% when "oneside" printing is selected, and to the left of all odd-numbered
% pages when "twoside" printing is selected
\setlength{\textwidth}{6.375in} % assuming US letter paper (8.5 in. x 11 in.) and 
% side margins as above
\raggedbottom

% The following statement specifies the amount of space between
% paragraphs. Other reasonable specifications are \bigskipamount and \smallskipamount.
\setlength{\parskip}{\medskipamount}

% The following statement controls the line spacing.  The default
% spacing corresponds to good typographic conventions and only slight
% changes (e.g., perhaps "1.2"), if any, should be made.
\renewcommand{\baselinestretch}{1} % this is the default line space setting

% By default, each chapter will start on a recto (right-hand side)
% page.  We also force each section of the front pages to start on 
% a recto page by inserting \cleardoublepage commands.
% In many cases, this will require that the verso page be
% blank and, while it should be counted, a page number should not be
% printed.  The following statements ensure a page number is not
% printed on an otherwise blank verso page.
\let\origdoublepage\cleardoublepage
\newcommand{\clearemptydoublepage}{%
  \clearpage{\pagestyle{empty}\origdoublepage}}
\let\cleardoublepage\clearemptydoublepage


%I have added these
\usepackage{times}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
%\usepackage[round]{natbib}

\DeclareMathOperator*{\argmin}{arg\,min}

\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem{note}{Note}

\newcommand{\mc}{\mathcal}
\newcommand{\mb}{\mathbf}
%======================================================================
%   L O G I C A L    D O C U M E N T -- the content of your thesis
%======================================================================
\begin{document}

% For a large document, it is a good idea to divide your thesis
% into several files, each one containing one chapter.
% To illustrate this idea, the "front pages" (i.e., title page,
% declaration, borrowers' page, abstract, acknowledgements,
% dedication, table of contents, list of tables, list of figures,
% nomenclature) are contained within the file "uw-ethesis-frontpgs.tex" which is
% included into the document by the following statement.
%----------------------------------------------------------------------
% FRONT MATERIAL
%----------------------------------------------------------------------
\input{uw-ethesis-frontpgs} 

%----------------------------------------------------------------------
% MAIN BODY
%----------------------------------------------------------------------
% Because this is a short document, and to reduce the number of files
% needed for this template, the chapters are not separate
% documents as suggested above, but you get the idea. If they were
% separate documents, they would each start with the \chapter command, i.e, 
% do not contain \documentclass or \begin{document} and \end{document} commands.
%======================================================================
\chapter{Introduction}
%======================================================================
\section{What is clustering?}
Computers `learn' in two ways. The first class of programs (or algorithms) are called \textit{supervised} methods where the algorithm learns to identify patterns (like sound or text of the alphabet 'A') with the help of a human being.  The second class of methods are \textit{unsupervised} where the algorithms learns to detect and distinguish patterns by themselves. 

Clustering refers to a class of unsupervised learning algorithms. Given a list of objects as input, clustering aims to group together `similar' objects while `separating' the dissimilar ones. As we can see, clustering is a vaguely defined task. The meaning of the term \textit{similar} can change with the intended application. We will discuss this issue in more detail in the next section. 

Note that a clustering algorithm takes as input a list of objects. This could be a list images or astronomical data (measurements from planets, stars etc.) or a list of DNA sequences or a list of users (of an online service) or a list of patients from a medical database or outputs from an industrial process or something else. This just goes to show the wide applicability of clustering algorithms. 

\section{Challenges}
\label{introduction:challenges}
The task of clustering is challenging due to the following impediments. 
\begin{itemize}
	\item \emph{Computationally expensive} - Clustering is often posed as an optimization problem. Given a list $\mc X$ of $n$ objects (or data points), the goal is to partition the list into say $k$ groups. A cost is associated with each possible partitioning of $\mc X$. The optimization problem then, is to find the partition with minimum cost. For many popular cost functions like $k$-means, $k$-median, $k$-medoids, it has been shown that finding the optimal solution is NP-Hard. \cite{dasgupta2008hardness,megiddo1984complexity}. 
	
	In practice, often heuristics are used. These heuristics are not guaranteed to find the optimal solution. Moreover, they can get stuck at locally optimal solutions. It is NP-Hard to approximate the optimal solution as well. 
	
	\item \emph{Under-specificity} - This is another major problem is clustering. One that has not received adequate attention from the research community. We mentioned in the previous section that the clustering problem is not precisely defined. The same dataset can be clustered in multiple different ways depending on the intended application. 
	
	Lets consider a concrete example. Consider a dataset of images of faces of people (both male and female) with different emotions (eg. laughing and crying). This toy dataset is shown in Fig. \ref{fig:laugh-cry}. Now, this dataset has two `correct' $2$-clusterings. One clustering is when the males are in one cluster and the females are in another.. Second clustering is when people laughing are in one cluster and the people crying are in another cluster. Both these are reasonable choices. 
	
	However, without extra information, the algorithm doesn't know which one it should prefer. It is possible that the clustering outputted by the algorithm is not what the `user' or the application desired. This is problem of under-specificity which arises whenever multiple solutions are possible. 
\end{itemize}

\section{Concrete questions}
Despite the challenges, clustering algorithms like $k$-means, $k$-median algorithms have been employed successfully in practice. Some applications include cancerous data detection, search engines etc. \cite{wang2005comparison,liu2007clustering}. 

Any satisfactory theory of clustering must examine and explain this gap between theory and practice. In particular

\begin{itemize}
	\item Under what conditions on data are $k$-means or $k$-median optimization problems easy to solve?
	\item Is it possible to have datasets on which popular clustering approaches fail but the proposed algorithm works?
	\item Are there clustering algorithms which take the structure of the data into account? When do these approaches work and when do they fail?
	\item How do you extract domain knowledge to better define the clustering task?
\end{itemize}

These are some of the questions that we would like to answer in our research. We have already made some progress on some of these questions. 

\section{Structure of the proposal}
In Chapter \ref{RLW}, we review some of the previous approaches in dealing with the challenges of clustering. In Chapter \ref{METHOD}, we introduce our notation and some definitions which will be used in the remainder of the proposal. In Chapters \ref{ANALYSIS} and \ref{SSC}, we discuss the approach we have taken to deal with the challenges in clustering. In Chapter \ref{FUTURE}, we discuss the avenues for future work. 

%============================================
\chapter{Related Work}
\label{RLW}
%=============================================
\section{Common previous approaches}
As discussed in Chapter \ref{introduction:challenges}, the most common approach to solving the clustering problem is to pose it as an optimization problem. Some of the examples include $k$-means optimization, $k$-median optimization, $k$-medoids etc. Below, we give a more formal definition of the $k$-means problem. 

\begin{definition}[$k$-means optimization]
Let $(\mb M, d)$ be a metric space. Given a finite set $\mc X \subseteq \mb M$ and $k$. Let $\mc C = \{C_1, \ldots. C_k\}$ be a partition of $\mc X$ into $k$ disjoint subsets. The goal is to find:
$$\argmin_{\mc C} \sum_{i=1}^k \sum_{x \in C_i} d^2(x, c_i)$$
where $c_i \in \mb M$ is the mean of all the points in $C_i$.  
\end{definition}

It has been shown that solving this optimization is NP-Hard \cite{dasgupta2008hardness}. In practice, heuristics are used to solve the minimization problem. The most popular is the Lloyd's algorithm \cite{lloyd1982least}. This algorithm has been used successfully on some clustering applications like cancerous data detection, search engines etc. \cite{wang2005comparison,liu2007clustering}. However, it is known that the Lloyd's algorithm can get stuck at local minima. It can sometimes even produce counter-intuitive clusterings \cite{mirkes2011k}. Similarly, other problems like $k$-median have also been shown to be NP-Hard \cite{megiddo1984complexity}.  

Another question that people have investigated is that ``Is it possible to find an approximation to the optimal solution?". In many cases, the answer to this question is also negative. $K$-Means is NP-Hard to approximate (Awasthi et. al \cite{awasthi2015hardness}). Constant factor approximation algorithms are known for the $k$-median problem. Arya et. al \cite{arya2004local} provide a $(3+\epsilon)$ approximation to the $k$-median cost. If the underlying metric is Euclidean, Kolliopoulos et. al \cite{kolliopoulos1999nearly} provide a $(1+\epsilon)$-approximation to the $k$-median cost. 

In majority of the cases, due to computational hardness, people have to rely on heuristics to solve the clustering problem. These heuristics have no performance guarantees and can often get stuck at local minima. 

\section{Dealing with computational hardness}
Despite the negative hardness results, clustering is applied successfully in practice. Researchers have examined this gap between theory and practice. One hypothesis that has recently gained traction is that ``Clustering is difficult only when it doesn't matter" or the CDNM thesis \cite{daniely2012clustering}. 

This means that the data on which clustering algorithms are computationally expensive do not occur in practice. Researchers have characterized such data sets by defining notions of \textit{clusterable} data. They then propose efficient clustering algorithms for these {\em nice} datasets.

Various notions of clusterability have been considered in the literature. Bilu et. al \cite{bilu2012stable} considered the problem of max-cut clustering of graphs. They defined an input instance to be `stable' if the optimal clustering doesn't change under small multiplicative perturbations. They designed an efficient clustering algorithm for $O(\sqrt{|\mc X|})$-resilient instances. 

Continuing the same line of work, Awasthi et. al \cite{awasthi2012center} defined the notion of $\alpha$-center proximity. Informally speaking, a clustering has $\alpha$-center proximity if any point is $\alpha$ times closer to its own cluster center than to any other center. Bigger the $\alpha$, more is the separation. If the $k$-median optimal solution satisfies $(\alpha > 3)$-center stability then Awasthi et. al \cite{awasthi2012center} give an efficient algorithm to find the optimal. Balcan et. al \cite{balcan2012clustering} then improved this result to $\alpha > \sqrt{2}+1$. The problem is NP-Hard for $\alpha < 2$ \cite{ben2014data}. 

Observe that the results discussed above assume that the entire input dataset has the niceness property. This assumption is very strict and unrealistic. A natural relaxation is to allow few points which do not have the niceness property. Such points are referred to as \textit{noise}. Balcan et. al \cite{balcan2012clustering} considered an input which has $\alpha$-center proximity except for an $\epsilon$ fraction of the points. For $\alpha > 2 + \sqrt{7}$, they were then able to find a $1+O(\epsilon)$-approximation to the cost of the $k$-median optimal solution. 

Ben-David et. al \cite{ben2014clustering} introduced a somewhat-related notion of $\lambda$-center separation. They showed that if the input has this property then they can convert any clustering algorithm to one which is robust to noise. Their framework is very general. They can `robustify' any center-based clustering algorithm. However, they require very strong conditions on the separability of the data as well as noise. 

In a different line of work, Ostrovsky et. al \cite{ostrovsky2006effectiveness} defined a notion of $\epsilon$-separatedness. An input satisfies this niceness property if the cost of the optimal $k$-clustering is $\epsilon^2$ times less than the cost of the optimal $(k-1)$-clustering. Ostrovsky et. al \cite{ostrovsky2006effectiveness} propose a variant of the Lloyd's algorithm. They show that if the input has $\epsilon$-separatedness then with high probability their algorithm finds a near-optimal $k$-means clustering.

Awasthi et. al \cite{awasthi2010stability} introduce a notion of \textit{weak deletion stability} (WDS). Informally speaking, it says that a clustering has $(1+\alpha)$-WDS if deleting one of the clusters increases the cost by a factor of atleast $(1+\alpha)$. If the data has this property then they propose an algorithm which finds a $(1+\epsilon)$-approximation to the cost of the optimal clustering in $O(k |\mc X|^{poly(1/\epsilon , 1/\alpha)})$ time. 

\section{Dealing with under-specificity}
As discussed in Chapter \ref{introduction:challenges}, another major challenge in clustering is under-specificity. Same dataset can have multiple `correct' clusterings. A clustering which is correct (suitable) for one application maybe counter-intuitive for another application. Without knowledge of the intended application, many popular clustering algorithms would fail on a lot of applications. This is probably one of the reasons that clustering is not as widely used as it should be.  In this section, we will discuss two approaches on dealing with under-specificity. 

\subsection{Multiple solutions} 
One simple approach is to allow the algorithm to output multiple solutions. A hierarchical clustering tree is one way to efficiently represent different `feasible' clusterings of the input dataset. The clustering of choice can then be found by efficiently searching over the different prunings of the tree. This approach has been taken Balcan et. al \cite{balcan2012clustering}. 

Another approach is to allow the algorithm to output a small list of what it considers feasible or good clusterings. Note that any list of size $O(|\mc X|^k)$ can output all the partitions of the dataset. Hence, any such list should have size polynomial in $k$ and $|\mc X|$.
   
\subsection{User interaction}
A more interesting approach is to incorporate domain expertise by allowing the algorithm to interact with or get advice from a user. This framework is sometimes referred to as \textbf{semi-supervised clustering}. The advice is aimed at getting rid of the ambiguity in clustering. 

This question has been addressed mainly in some application-oriented works. The algorithm gets as input, a dataset $\mc X$. In addition, \textit{link/do-not-link} constraints are also provided by the user. These constraints basically say that a particular pair of input instances should (or shouldn't) belong to the same cluster. Basu et. al \cite{basu2002semi} used these constraints to help guide the choice of initial centers in the Lloyd algorithm. 

Another way to incorporate supervision is to assume that the data is generated from a particular distribution. The task is then to estimate the parameters of this distribution (\cite{basu2004probabilistic, kulis2009semi}). Ashtiani et. al \cite{ashtiani2015representation} incorporated supervision by asking the user or expert to cluster a small subset of the dataset. Using this information, their algorithm tries to infer the `true' clustering. 

Balcan et. al \cite{balcan2008clustering} considered an interactive model of clustering. Their framework works as follows. The expert is provided with the current clustering. The expert then responds back by suggesting to either split a cluster or merge two clusters. The authors investigate the computational and query complexity of this problem. 
%======================================================================
\chapter{Preliminaries and Notation}
\label{METHOD}
%======================================================================
\section{Definition}
\section{Clusterability notions}
	\subsection{$\alpha$-center proximity}
	\subsection{$\gamma$-margin}
	\subsection{$\lambda$-center separation}
\section{Same-cluster queries}


%======================================================================
\chapter{Sparse Noise}
\label{ANALYSIS}
%======================================================================
\section{Center Proximity}
\subsection{Positive result}
\subsection{Negative result}
\section{Center Separation}
\subsection{Positive result}
\subsection{Negative result}


%======================================================================
\chapter{Sem-supervised Clustering}
\label{SSC}
%======================================================================
\section{Positive result}
\section{Negative result}


%======================================================================
\chapter{Future Plans}
\label{FUTURE}
%======================================================================












%=============================================================================

\appendix

% Add a title page before the appendices and a line in the Table of Contents

%%======================================================================
%\chapter[PDF Plots From Matlab]{Matlab Code for Making a PDF Plot}
%\label{AppendixA}
%% Tip 4: Example of how to get a shorter chapter title for the Table of Contents 
%%======================================================================
%\section{Using the GUI}
%Properties of Matab plots can be adjusted from the plot window via a graphical interface. Under the Desktop menu in the Figure window, select the Property Editor. You may also want to check the Plot Browser and Figure Palette for more tools. To adjust properties of the axes, look under the Edit menu and select Axes Properties.
%
%To set the figure size and to save as PDF or other file formats, click the Export Setup button in the figure Property Editor.
%
%\section{From the Command Line} 
%All figure properties can also be manipulated from the command line. Here's an example: 
%\begin{verbatim}
%x=[0:0.1:pi];
%hold on % Plot multiple traces on one figure
%plot(x,sin(x))
%plot(x,cos(x),'--r')
%plot(x,tan(x),'.-g')
%title('Some Trig Functions Over 0 to \pi') % Note LaTeX markup!
%legend('{\it sin}(x)','{\it cos}(x)','{\it tan}(x)')
%hold off
%set(gca,'Ylim',[-3 3]) % Adjust Y limits of "current axes"
%set(gcf,'Units','inches') % Set figure size units of "current figure"
%set(gcf,'Position',[0,0,6,4]) % Set figure width (6 in.) and height (4 in.)
%cd n:\thesis\plots % Select where to save
%print -dpdf plot.pdf % Save as PDF
%\end{verbatim}

%----------------------------------------------------------------------
% END MATERIAL
%----------------------------------------------------------------------

% B I B L I O G R A P H Y
% -----------------------

% The following statement selects the style to use for references.  It controls the sort order of the entries in the bibliography and also the formatting for the in-text labels.
\bibliographystyle{plain}
% This specifies the location of the file containing the bibliographic information.  
% It assumes you're using BibTeX (if not, why not?).
\cleardoublepage % This is needed if the book class is used, to place the anchor in the correct page,
                 % because the bibliography will start on its own page.
                 % Use \clearpage instead if the document class uses the "oneside" argument
\phantomsection  % With hyperref package, enables hyperlinking from the table of contents to bibliography             
% The following statement causes the title "References" to be used for the bibliography section:
\renewcommand*{\bibname}{References}

% Add the References to the Table of Contents
\addcontentsline{toc}{chapter}{\textbf{References}}

\bibliography{proposal}
% Tip 5: You can create multiple .bib files to organize your references. 
% Just list them all in the \bibliogaphy command, separated by commas (no spaces).

% The following statement causes the specified references to be added to the bibliography% even if they were not 
% cited in the text. The asterisk is a wildcard that causes all entries in the bibliographic database to be included (optional).
\nocite{*}

\end{document}
